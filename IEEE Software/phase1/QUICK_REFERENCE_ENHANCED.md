# Phase 1 Enhanced Experiments - Quick Reference

## üöÄ Running the Full Pipeline

```powershell
# Navigate to phase1 directory
cd "IEEE Software\phase1"

# 1. Create data splits (only needed once)
python src/data_utils.py

# 2. Run enhanced experiments with statistical tests
python run_enhanced_experiments.py

# 3. Generate visualizations with error bars
python visualize_enhanced_results.py

# 4. Generate production cost analysis
python analyze_production_costs.py

# 5. Update reproducibility doc with system specs
python update_reproducibility.py
```

**Total Runtime**: ~2 minutes for full pipeline

---

## üìä Key Results at a Glance

### Performance (Test Set: 400 samples)

| Defense | TPR | FPR | F1 | CI Width |
|---------|-----|-----|----|----------|
| **Signature-Only** | 25.0% | 0.0% | 0.400 | ¬±6% |
| **Rules-Only** | 24.0% | 1.5% | 0.382 | ¬±6% |
| **NeMo-Baseline** | 12.5% | 1.0% | 0.220 | ¬±5% |

### Statistical Significance

- ‚úÖ **Signature > NeMo** (p=0.0006)
- ‚úÖ **Rules > NeMo** (p=0.0054)
- ‚ùå **Signature = Rules** (p=0.6025)

### Production Cost (1% prevalence, 10k requests/day)

- **Signature-Only**: 0 false alarms, 75 missed attacks
- **Rules-Only**: 148.5 false alarms, 76 missed attacks  
- **NeMo-Baseline**: 99 false alarms, 87.5 missed attacks

**Winner**: Signature-Only (0% FPR eliminates user friction)

---

## üìÅ Output Files

### Results (in `results/` directory)
```
phase1_test_results.json              # Detailed results with CIs
phase1_test_summary.csv                # Quick summary table
phase1_mcnemar_tests.csv               # Statistical significance
phase1_family_analysis.csv             # Per-family performance
phase1_cost_analysis.csv               # Cost analysis data
phase1_performance_with_error_bars.png # Main visualization
phase1_roc_comparison.png              # ROC scatter plot
phase1_family_heatmap.png              # Family performance
phase1_cost_analysis.png               # Cost plots
phase1_test_table.tex                  # McNemar LaTeX table
phase1_cost_table.tex                  # Cost LaTeX table
```

### Data (in `data/splits/` directory)
```
train.csv          # 1,000 samples (for future pattern discovery)
dev.csv            # 400 samples (for future threshold tuning)
test.csv           # 400 samples (used for all Phase 1 results)
ood.csv            # 200 samples (for future generalization testing)
split_metadata.json # Split configuration
```

---

## üîë Key Findings

### 1. Input Detection Performance Gap
- **Original** (output detection): Signature-Only ~80% TPR
- **Enhanced** (input detection): Signature-Only ~25% TPR
- **Reason**: Detecting attacks in prompts (input) is harder than detecting leaked tokens in responses (output)

### 2. Attack Family Performance

| Family | % of Attacks | Signature TPR | Rules TPR | NeMo TPR |
|--------|--------------|---------------|-----------|----------|
| exfiltration | 6.4% | **100%** | **100%** | 15.4% |
| instruction_override | 20.4% | 48.8% | 31.7% | 26.8% |
| other_attack | 73.2% | 11.6% | 15.1% | 8.2% |

**Blind Spot**: 73% of attacks fall in "other_attack" category with <15% TPR across all defenses.

### 3. Production FPR Impact

At 1% attack prevalence (realistic for most systems):
- **0% FPR** (Signature): 0 blocked users
- **1.5% FPR** (Rules): **~149 blocked users per 10k requests**
- **1.0% FPR** (NeMo): ~99 blocked users per 10k requests

**Implication**: Even small FPR has massive impact at low prevalence.

### 4. Statistical Significance
- Signature and Rules significantly outperform NeMo (p<0.01)
- No significant difference between Signature and Rules (p=0.60)
- **Recommendation**: Choose Signature-Only for production (0% FPR advantage)

---

## üéØ For IEEE Software Article

### What to Emphasize

1. **Methodological Rigor**:
   - "All results evaluated on held-out test set to prevent optimistic bias"
   - "Statistical significance assessed via McNemar tests with Bonferroni correction"
   - "95% confidence intervals computed via bootstrap (n=1,000)"

2. **Production Realism**:
   - "At 1% attack prevalence, Rules-Only blocks 149 legitimate users per 10k requests vs Signature-Only's 0"
   - "FPR dominates cost at realistic attack prevalence (<5%)"

3. **Attack Family Analysis**:
   - "Defenses excel at exfiltration (100% TPR) but struggle with 'other' attacks (8-15% TPR)"
   - "73% of attacks fall outside known pattern families"

4. **Reproducibility**:
   - "Full experimental protocol documented with system specs, package versions, and random seeds"
   - "Standalone implementation available for journal replication"

### Tables for Paper

**Table 1**: Baseline Performance (use `phase1_test_summary.csv`)
- Columns: Defense | TPR [95% CI] | FPR [95% CI] | F1 [95% CI] | Latency

**Table 2**: Statistical Significance (use `phase1_mcnemar_tests.csv`)
- Columns: Comparison | Acc(A) | Acc(B) | p-value | Significant?

**Table 3**: Production Cost Analysis (use `phase1_cost_table.tex`)
- Columns: Defense | TPR | FPR | False Alarms/10k | Missed Attacks/10k | Net Value

**Figure 1**: Use `phase1_performance_with_error_bars.png`
**Figure 2**: Use `phase1_family_heatmap.png`
**Figure 3**: Use `phase1_cost_analysis.png`

---

## üîß Modifying Experiments

### Change Prevalence Levels for Cost Analysis

Edit `analyze_production_costs.py` line 185:
```python
prevalence_levels = [0.001, 0.005, 0.01, 0.02, 0.05]  # Modify these values
```

### Change Cost/Benefit Values

Edit `analyze_production_costs.py` function `calculate_production_costs()`:
```python
fp_cost: float = 1.0,     # Cost of blocking legitimate user
fn_cost: float = 10.0,    # Cost of missing an attack
tp_benefit: float = 9.0   # Benefit of catching an attack
```

### Change Confidence Level

Edit `run_enhanced_experiments.py` line 48:
```python
confidence: float = 0.95  # Change to 0.90 for 90% CI, 0.99 for 99% CI
```

### Change Bootstrap Iterations

Edit `run_enhanced_experiments.py` line 47:
```python
n_bootstrap: int = 1000  # Increase for more precision, decrease for speed
```

### Add More Defenses

Edit `run_enhanced_experiments.py` lines 238-242:
```python
defenses = {
    'Signature-Only': SignatureOnlyDefense(),
    'Rules-Only': RulesOnlyDefense('configs/rules.yml'),
    'NeMo-Baseline': NeMoBaselineDefense(threshold=0.5),
    'Your-Defense': YourDefenseClass(),  # Add here
}
```

---

## üìö Documentation Files

1. **FINAL_RESULTS.md** (this file) - Complete results summary
2. **REPRODUCIBILITY.md** - Full reproducibility documentation
3. **ENHANCEMENTS_SUMMARY.md** - Implementation details
4. **README.md** - Original Phase 1 documentation
5. **QUICKSTART.md** - Original quick start guide

---

## ‚úÖ Verification

After running the full pipeline, verify:

1. **Results files exist**: Check `results/` directory for 11 files
2. **Visualizations generated**: Check for 4 PNG files in `results/`
3. **Data splits created**: Check `data/splits/` for 5 files
4. **No errors in output**: All scripts should complete with "‚úÖ" message
5. **McNemar p-values make sense**: Signature vs NeMo should be <0.05
6. **CIs don't overlap zero**: All metrics should have positive lower bounds (except FPR which can be 0)

---

## üêõ Common Issues

### Issue: "ModuleNotFoundError: No module named 'yaml'"
**Solution**: `pip install pyyaml`

### Issue: "ModuleNotFoundError: No module named 'scipy.stats.contingency'"
**Solution**: Upgrade scipy: `pip install --upgrade scipy`

### Issue: "File not found: data/prompts_hf_augmented.csv"
**Solution**: Copy dataset from parent directory:
```powershell
Copy-Item "../data/prompts_hf_augmented.csv" -Destination "data/"
```

### Issue: McNemar test errors with "'float' and 'str'"
**Solution**: Already fixed - NeMo threshold must be float (0.5), not string

### Issue: Bootstrap taking too long
**Solution**: Reduce n_bootstrap from 1000 to 100 in `run_enhanced_experiments.py`

---

## üí° Tips

1. **Run experiments in order**: Data splits ‚Üí Experiments ‚Üí Visualizations ‚Üí Cost analysis
2. **Check results/*.json first**: Contains all raw data if you need to debug
3. **LaTeX tables**: Ready to copy-paste into Overleaf for paper draft
4. **PNG resolution**: All plots saved at 300 DPI (publication quality)
5. **Reproducibility**: Random seed 42 used throughout for consistent results

---

## üìû Support

For questions about:
- **Statistical tests**: See `src/statistical_tests.py` docstrings
- **Cost analysis**: See `analyze_production_costs.py` docstrings  
- **Data splits**: See `src/data_utils.py` docstrings
- **Visualizations**: See `visualize_enhanced_results.py` docstrings

---

**Last Updated**: 2025-10-29  
**Status**: ‚úÖ All experiments complete  
**Next Phase**: Phase 2 - Advanced defenses with ML
